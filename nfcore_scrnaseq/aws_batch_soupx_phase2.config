/*
 * nf-core/scrnaseq AWS Batch Configuration - SoupX Variant (Phase 2)
 *
 * APPROVED CONFIGURATION for AWS testing:
 * ✓ SoupX enabled (contamination removal, lightweight)
 * ✓ CellBender disabled (colleague request: SoupX only)
 * ✓ EmptyDrops disabled (filtering handled by SoupX)
 * ✓ Optimized for spot instances (cost control)
 * ✓ Matches local HPC validated parameters
 *
 * Before running: Replace all YOUR_* placeholders with actual values
 * See: AWS_PHASE2_CHECKLIST.md for step-by-step setup
 */

params {
    // ============================================================================
    // INPUT/OUTPUT - S3 PATHS (REPLACE YOUR_BUCKET)
    // ============================================================================
    // Example: s3://scrnaseq-phase2-1706345678/samplesheet_phase2.csv

    input = 's3://YOUR_BUCKET/samplesheet_phase2.csv'
    outdir = 's3://YOUR_BUCKET/results_phase2'

    // ============================================================================
    // REFERENCE FILES - S3 PATHS (REPLACE YOUR_BUCKET)
    // ============================================================================
    // Upload with: aws s3 cp local_file s3://YOUR_BUCKET/references/

    // Splici reference (spliced + intronic transcriptome)
    // Size: ~5 GB
    txp2gene = 's3://YOUR_BUCKET/references/splici_fl86_t2g_3col.tsv'

    // Source genome reference (optional if using iGenomes)
    // Size: ~3 GB
    fasta = 's3://YOUR_BUCKET/references/genome.fa'
    gtf = 's3://YOUR_BUCKET/references/genes.gtf'

    // ============================================================================
    // PIPELINE SETTINGS (Validated from Local HPC Testing)
    // ============================================================================

    // Aligner: alevin-fry via simpleaf wrapper (8 min local, proven)
    aligner = 'alevin'

    // Chemistry: 10x Chromium v3 (TSP1_lung_1 test sample)
    protocol = '10XV3'

    // Read length: 101 bp raw, configured as 91 bp (barcode handling)
    simpleaf_rlen = 91

    // ============================================================================
    // FILTERING STRATEGY (SoupX Only - Per Colleague Request)
    // ============================================================================

    // EmptyDrops: DISABLED (colleague said SoupX is sufficient)
    // Default: true, but we skip it to match local SoupX-only run
    skip_emptydrops = true

    // CellBender: DISABLED (colleague explicit request: "No CellBender is needed")
    // Reason:
    //   1. CellBender failed locally due to Singularity permission issues
    //   2. Colleague explicitly requests SoupX only
    //   3. SoupX performs well, 20 min runtime, 7,498 cells validated
    skip_cellbender = true

    // ============================================================================
    // REFERENCE MANAGEMENT
    // ============================================================================

    // Save splici reference in results for reuse on future runs
    // Useful if processing multiple samples
    save_reference = true

    // ============================================================================
    // RESOURCE LIMITS
    // ============================================================================
    // Conservative allocation - actual pipeline uses much less
    // Local HPC peak: 1.1 GB memory, 8 CPUs
    // AWS allocation accounts for parallel processes

    max_cpus = 16           // Enough for SIMPLEAF_QUANT (bottleneck)
    max_memory = '64.GB'    // Conservative for all processes
    max_time = '12.h'       // Generous limit to avoid timeouts
}

// ============================================================================
// AWS BATCH EXECUTOR CONFIGURATION
// ============================================================================

process {
    executor = 'awsbatch'

    // REQUIRED: Replace with your actual CPU queue
    // Get your queue name from: aws batch describe-job-queues
    // Example: scrnaseq-cpu-queue, default-queue, etc.
    queue = 'YOUR_CPU_QUEUE'

    // ========================================================================
    // DEFAULT PROCESS RESOURCES
    // ========================================================================

    cpus = 4
    memory = '16.GB'
    time = '4.h'

    // ========================================================================
    // PROCESS LABELS (Predefined resource tiers)
    // ========================================================================

    // High-intensity processes (index building, quantification)
    withLabel: 'process_high' {
        cpus = 16
        memory = '48.GB'
        time = '6.h'
    }

    // Medium-intensity processes
    withLabel: 'process_medium' {
        cpus = 8
        memory = '32.GB'
        time = '4.h'
    }

    // Low-intensity processes (QC, conversion)
    withLabel: 'process_low' {
        cpus = 2
        memory = '8.GB'
        time = '2.h'
    }

    // ========================================================================
    // PROCESS-SPECIFIC RESOURCES
    // ========================================================================

    // FASTQC: Quality control (parallel-friendly, ~10 min)
    withName: 'FASTQC' {
        cpus = 8
        memory = '16.GB'
        time = '3.h'
    }

    // SIMPLEAF_INDEX: Build transcriptome index (~30 min)
    // This is memory-intensive due to k-mer indexing
    withName: 'SIMPLEAF_INDEX' {
        cpus = 16
        memory = '48.GB'
        time = '3.h'
    }

    // SIMPLEAF_QUANT: Main quantification step (~20 min - the bottleneck)
    // This is the slowest step; giving full resources helps
    // Local: 1.1 GB peak, so 48 GB is plenty
    withName: 'SIMPLEAF_QUANT' {
        cpus = 16
        memory = '48.GB'
        time = '2.h'
    }

    // AlevinQC: Quality control plots
    withName: 'ALEVINQC' {
        cpus = 2
        memory = '8.GB'
        time = '1.h'
    }

    // Format conversions: MTX to H5AD, Seurat, SingleCellExperiment
    withName: 'MTX_TO_H5AD|MTX_TO_SEURAT|MTX_TO_SCE' {
        cpus = 2
        memory = '8.GB'
        time = '1.h'
    }

    // MultiQC: Summary report generation
    withName: 'MULTIQC' {
        cpus = 2
        memory = '4.GB'
        time = '1.h'
    }
}

// ============================================================================
// AWS BATCH SETTINGS
// ============================================================================

aws {
    // AWS Region - Update if not using us-east-1
    // Region should match where your S3 bucket is located
    // To check: aws s3api get-bucket-location --bucket YOUR_BUCKET
    region = 'us-east-1'

    // AWS Batch CLI path (standard for ECS/Batch)
    batch {
        cliPath = '/home/ec2-user/miniconda/bin/aws'
    }
}

// ============================================================================
// CONTAINER CONFIGURATION
// ============================================================================

// Docker required for AWS Batch (Singularity not available in AWS)
docker {
    enabled = true
}

// Optional: Wave container registry caching (set to true for faster pulls)
// Only enable if Wave is configured in your AWS account
wave {
    enabled = false
}

// ============================================================================
// NEXTFLOW CONFIGURATION
// ============================================================================

// Nextflow tower (optional - for monitoring via web interface)
// Uncomment if you have tower token configured
// tower {
//     accessToken = 'YOUR_TOWER_TOKEN'
//     enabled = true
// }

// ============================================================================
// COST OPTIMIZATION NOTES
// ============================================================================

/*
IMPORTANT FOR PHASE 2 COST CONTROL:

1. Data Transfer Costs:
   - FASTQ upload: ~40 GB × $0.02/GB = $0.80
   - Results download: ~200 MB × free (outbound to local) = $0
   - Total: ~$0.80

2. Compute Costs (with spot instances ~60% discount):
   - Without discount: ~$18/job
   - With spot: ~$5-7/job
   - Better rate if using reserved instances

3. Storage Costs:
   - Input FASTQ: $0.023/GB/month (small impact)
   - Work directory: DELETE IMMEDIATELY after run! ($11/month if left)
   - Results: Keep for reference

4. Total Phase 2 Test: $0.50-0.70 per sample

CLEANUP AFTER COMPLETION:
aws s3 rm s3://YOUR_BUCKET/work_phase2 --recursive
*/

// ============================================================================
// TROUBLESHOOTING NOTES
// ============================================================================

/*
If you encounter issues:

1. Queue Not Found:
   Error: "Queue not found: YOUR_CPU_QUEUE"
   Fix: Run "aws batch describe-job-queues" and use actual queue name

2. Access Denied to S3:
   Error: "Access Denied" when accessing S3
   Fix: Check IAM role - ensure EC2 instances have S3 read/write permissions

3. Docker Image Pull Failed:
   Error: "Failed to pull image"
   Fix: Check internet connectivity in Batch compute environment

4. Out of Capacity:
   Error: "Job failed - not enough capacity"
   Fix: Use smaller instance types or wait for capacity

For debugging:
- CloudWatch logs: aws logs tail /aws/batch/job --follow
- Job details: aws batch describe-jobs --jobs JOB_ID
- Check results: aws s3 ls s3://YOUR_BUCKET/results_phase2/
*/
